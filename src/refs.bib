%------ Atleast cite ü§û ~60 . paligemma does (135)

% convolutional sequence to sequence
@misc{gehring2017convolutionalsequencesequencelearning,
      title={Convolutional Sequence to Sequence Learning}, 
      author={Jonas Gehring and Michael Auli and David Grangier and Denis Yarats and Yann N. Dauphin},
      year={2017},
      eprint={1705.03122},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1705.03122}, 
}

%all you need
@misc{vaswani2023attentionneed,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2023},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1706.03762}, 
}

% annotated transformers
@misc{annotatedtransformer,
  author       = {Sasha Rush ,  Austin Huang, Suraj Subramanian, Jonathan Sum, Khalid Almubarak, and Stella Biderman.},
  title        = {The Annotated Transformer},
  howpublished = {\url{https://nlp.seas.harvard.edu/annotated-transformer/}},
  year         = {2018},
  note         = {Accessed: 2025-01-16}
}

% self attention produces disjointed saliency
@misc{zhang2024depthwiseconvolutionsvisiontransformers,
      title={Depth-Wise Convolutions in Vision Transformers for Efficient Training on Small Datasets}, 
      author={Tianxiao Zhang and Wenju Xu and Bo Luo and Guanghui Wang},
      year={2024},
      eprint={2407.19394},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2407.19394}, 
}
% slaiency != explainability
@article{bertrand2022saliency,
   title={Searching for Unintended Biases with Saliency},
   author={Bertrand, Astrid and Pearce, Adam and Thain, Nithum},
   year={2022},
   journal={PAIR Explorables},
   note={https://pair.withgoogle.com/explorables/saliency/}
}

% vits are inherently saliency learners
@inproceedings{DJILALI_2023_BMVC_inherent_saliency,
author    = {YASSER ABDELAZIZ DAHOU DJILALI and Kevin McGuinness and Noel O Connor},
title     = {Vision Transformers are Inherently Saliency Learners},
booktitle = {34th British Machine Vision Conference 2023, {BMVC} 2023, Aberdeen, UK, November 20-24, 2023},
publisher = {BMVA},
year      = {2023},
url       = {https://papers.bmvc2023.org/0771.pdf}
}



% imaage is worth 16x16
@misc{dosovitskiy2021imageworth16x16words,
      title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale}, 
      author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
      year={2021},
      eprint={2010.11929},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2010.11929}, 
}

% shape optimal ViT
@misc{alabdulmohsin2024gettingvitshapescaling,
      title={Getting ViT in Shape: Scaling Laws for Compute-Optimal Model Design}, 
      author={Ibrahim Alabdulmohsin and Xiaohua Zhai and Alexander Kolesnikov and Lucas Beyer},
      year={2024},
      eprint={2305.13035},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2305.13035}, 
}
 %paligemma
@article{beyer2024paligemma,
    title={{PaliGemma: A versatile 3B VLM for transfer}},
    author={Lucas Beyer* and Andreas Steiner* and Andr√© Susano Pinto* and Alexander Kolesnikov* and Xiao Wang* and Daniel Salz and Maxim Neumann and Ibrahim Alabdulmohsin and Michael Tschannen and Emanuele Bugliarello and Thomas Unterthiner and Daniel Keysers and Skanda Koppula and Fangyu Liu and Adam Grycner and Alexey Gritsenko and Neil Houlsby and Manoj Kumar and Keran Rong and Julian Eisenschlos and Rishabh Kabra and Matthias Bauer and Matko Bo≈°njak and Xi Chen and Matthias Minderer and Paul Voigtlaender and Ioana Bica and Ivana Balazevic and Joan Puigcerver and Pinelopi Papalampidi and Olivier Henaff and Xi Xiong and Radu Soricut and Jeremiah Harmsen and Xiaohua Zhai*},
    year={2024},
    journal={arXiv preprint arXiv:2407.07726}
}

%microsoft phi3
@techreport{abdin2024phi,
author = {Abdin, Marah I and Ade Jacobs, Sam and Awan, Ammar Ahmad and Aneja, Jyoti and Awadallah, Ahmed and Hassan Awadalla, Hany and Bach, Nguyen and Bahree, Amit and Bakhtiari, Arash and Behl, Harkirat and Benhaim, Alon and Bilenko, Misha and Bjorck, Johan and Bubeck, S√©bastien and Cai, Martin and Mendes, Caio C√©sar Teodoro and Chen, Weizhu and Chaudhary, Vishrav and Chopra, Parul and Giorno, Allie Del and de Rosa, Gustavo and Dixon, Matthew and Eldan, Ronen and Iter, Dan and Goswami, Abhishek and Gunasekar, Suriya and Haider, Emman and Hao, Junheng and Russell J. Hewett and Huynh, Jamie and Javaheripi, Mojan and Jin, Xin and Kauffmann, Piero and Karampatziakis, Nikos and Kim, Dongwoo and Khademi, Mahmoud and Kurilenko, Lev and Lee, James R. and Lee, Yin Tat and Li, Yuanzhi and Liang, Chen and Liu, Weishung and Lin, Xihui (Eric) and Lin, Zeqi and Madan, Piyush and Mitra, Arindam and Modi, Hardik and Nguyen, Anh and Norick, Brandon and Patra, Barun and Perez-Becker, Daniel and Portet, Thomas and Pryzant, Reid and Qin, Heyang and Radmilac, Marko and Rosset, Corby and Roy, Sambudha and Saarikivi, Olli and Saied, Amin and Salim, Adil and Santacroce, Michael and Shah, Shital and Shang, Ning and Sharma, Hiteshi and Song, Xia and Ruwase, Olatunji and Wang, Xin and Ward, Rachel and Wang, Guanhua and Witte, Philipp and Wyatt, Michael and Xu, Can and Xu, Jiahang and Xu, Weijian and Yadav, Sonali and Yang, Fan and Yang, Ziyi and Yu, Donghan and Zhang, Chengruidong and Zhang, Cyril and Zhang, Jianwen and Zhang, Li Lyna and Zhang, Yi and Zhang, Yunan and Zhou, Xiren},
title = {Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone},
institution = {Microsoft},
year = {2024},
month = {August},
abstract = {We introduce phi-3-mini, a 3.8 billion parameter language model trained on 3.3 trillion tokens, whose overall performance, as measured by both academic benchmarks and internal testing, rivals that of models such as Mixtral 8x7B and GPT-3.5 (e.g., phi-3-mini achieves 69% on MMLU and 8.38 on MT-bench), despite being small enough to be deployed on a phone. The innovation lies entirely in our dataset for training, a scaled-up version of the one used for phi-2, composed of heavily filtered web data and synthetic data. The model is also further aligned for robustness, safety, and chat format. We also provide some initial parameter-scaling results with a 7B and 14B models trained for 4.8T tokens, called phi-3-small and phi-3-medium, both significantly more capable than phi-3-mini (e.g., respectively 75% and 78% on MMLU, and 8.7 and 8.9 on MT-bench).},
url = {https://www.microsoft.com/en-us/research/publication/phi-3-technical-report-a-highly-capable-language-model-locally-on-your-phone/},
number = {MSR-TR-2024-12},
}


%siglip
@misc{zhai2023sigmoidlosslanguageimage,
      title={Sigmoid Loss for Language Image Pre-Training}, 
      author={Xiaohua Zhai and Basil Mustafa and Alexander Kolesnikov and Lucas Beyer},
      year={2023},
      eprint={2303.15343},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2303.15343}, 
}

%gemma2b
@misc{gemmateam2024gemma2improvingopen,
      title={Gemma 2: Improving Open Language Models at a Practical Size}, 
      author={Gemma Team and Morgane Riviere and Shreya Pathak and Pier Giuseppe Sessa and Cassidy Hardin and Surya Bhupatiraju and L√©onard Hussenot and Thomas Mesnard and Bobak Shahriari and Alexandre Ram√© and Johan Ferret and Peter Liu and Pouya Tafti and Abe Friesen and Michelle Casbon and Sabela Ramos and Ravin Kumar and Charline Le Lan and Sammy Jerome and Anton Tsitsulin and Nino Vieillard and Piotr Stanczyk and Sertan Girgin and Nikola Momchev and Matt Hoffman and Shantanu Thakoor and Jean-Bastien Grill and Behnam Neyshabur and Olivier Bachem and Alanna Walton and Aliaksei Severyn and Alicia Parrish and Aliya Ahmad and Allen Hutchison and Alvin Abdagic and Amanda Carl and Amy Shen and Andy Brock and Andy Coenen and Anthony Laforge and Antonia Paterson and Ben Bastian and Bilal Piot and Bo Wu and Brandon Royal and Charlie Chen and Chintu Kumar and Chris Perry and Chris Welty and Christopher A. Choquette-Choo and Danila Sinopalnikov and David Weinberger and Dimple Vijaykumar and Dominika Rogozi≈Ñska and Dustin Herbison and Elisa Bandy and Emma Wang and Eric Noland and Erica Moreira and Evan Senter and Evgenii Eltyshev and Francesco Visin and Gabriel Rasskin and Gary Wei and Glenn Cameron and Gus Martins and Hadi Hashemi and Hanna Klimczak-Pluci≈Ñska and Harleen Batra and Harsh Dhand and Ivan Nardini and Jacinda Mein and Jack Zhou and James Svensson and Jeff Stanway and Jetha Chan and Jin Peng Zhou and Joana Carrasqueira and Joana Iljazi and Jocelyn Becker and Joe Fernandez and Joost van Amersfoort and Josh Gordon and Josh Lipschultz and Josh Newlan and Ju-yeong Ji and Kareem Mohamed and Kartikeya Badola and Kat Black and Katie Millican and Keelin McDonell and Kelvin Nguyen and Kiranbir Sodhia and Kish Greene and Lars Lowe Sjoesund and Lauren Usui and Laurent Sifre and Lena Heuermann and Leticia Lago and Lilly McNealus and Livio Baldini Soares and Logan Kilpatrick and Lucas Dixon and Luciano Martins and Machel Reid and Manvinder Singh and Mark Iverson and Martin G√∂rner and Mat Velloso and Mateo Wirth and Matt Davidow and Matt Miller and Matthew Rahtz and Matthew Watson and Meg Risdal and Mehran Kazemi and Michael Moynihan and Ming Zhang and Minsuk Kahng and Minwoo Park and Mofi Rahman and Mohit Khatwani and Natalie Dao and Nenshad Bardoliwalla and Nesh Devanathan and Neta Dumai and Nilay Chauhan and Oscar Wahltinez and Pankil Botarda and Parker Barnes and Paul Barham and Paul Michel and Pengchong Jin and Petko Georgiev and Phil Culliton and Pradeep Kuppala and Ramona Comanescu and Ramona Merhej and Reena Jana and Reza Ardeshir Rokni and Rishabh Agarwal and Ryan Mullins and Samaneh Saadat and Sara Mc Carthy and Sarah Cogan and Sarah Perrin and S√©bastien M. R. Arnold and Sebastian Krause and Shengyang Dai and Shruti Garg and Shruti Sheth and Sue Ronstrom and Susan Chan and Timothy Jordan and Ting Yu and Tom Eccles and Tom Hennigan and Tomas Kocisky and Tulsee Doshi and Vihan Jain and Vikas Yadav and Vilobh Meshram and Vishal Dharmadhikari and Warren Barkley and Wei Wei and Wenming Ye and Woohyun Han and Woosuk Kwon and Xiang Xu and Zhe Shen and Zhitao Gong and Zichuan Wei and Victor Cotruta and Phoebe Kirk and Anand Rao and Minh Giang and Ludovic Peran and Tris Warkentin and Eli Collins and Joelle Barral and Zoubin Ghahramani and Raia Hadsell and D. Sculley and Jeanine Banks and Anca Dragan and Slav Petrov and Oriol Vinyals and Jeff Dean and Demis Hassabis and Koray Kavukcuoglu and Clement Farabet and Elena Buchatskaya and Sebastian Borgeaud and Noah Fiedel and Armand Joulin and Kathleen Kenealy and Robert Dadashi and Alek Andreev},
      year={2024},
      eprint={2408.00118},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2408.00118}, 
}


%llama 3.1
@misc{touvron2023llamaopenefficientfoundation,
      title={LLaMA: Open and Efficient Foundation Language Models}, 
      author={Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timoth√©e Lacroix and Baptiste Rozi√®re and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
      year={2023},
      eprint={2302.13971},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2302.13971}, 
}

% tokenizer : sentencepiece
@misc{kudo2018sentencepiecesimplelanguageindependent,
      title={SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing}, 
      author={Taku Kudo and John Richardson},
      year={2018},
      eprint={1808.06226},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1808.06226}, 
}

% baby doctor github : individual researcher 1
@misc{photomzroco2023,
  author = {Markus Zhang, Vir Chau},
  title = {BabyDoctor},
  year = {2023},
  howpublished = {\url{https://github.com/photomz/BabyDoctor}},
  note = {GitHub}
}

%individual researcher2
@misc{vansonsbeek2023openendedmedicalvisualquestion,
      title={Open-Ended Medical Visual Question Answering Through Prefix Tuning of Language Models}, 
      author={Tom van Sonsbeek and Mohammad Mahdi Derakhshani and Ivona Najdenkoska and Cees G. M. Snoek and Marcel Worring},
      year={2023},
      eprint={2303.05977},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2303.05977}, 
}

% fairface
@inproceedings{karkkainenfairface,
  title={FairFace: Face Attribute Dataset for Balanced Race, Gender, and Age for Bias Measurement and Mitigation},
  author={Karkkainen, Kimmo and Joo, Jungseock},
  booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  year={2021},
  pages={1548--1558}
}

% cc12 m instruct
@inproceedings{changpinyo2021cc12m,
  title = {{Conceptual 12M}: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts},
  author = {Changpinyo, Soravit and Sharma, Piyush and Ding, Nan and Soricut, Radu},
  booktitle = {CVPR},
  year = {2021},
}

% conceptual captions
@inproceedings{sharma2018conceptual,
  title = {Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning},
  author = {Sharma, Piyush and Ding, Nan and Goodman, Sebastian and Soricut, Radu},
  booktitle = {Proceedings of ACL},
  year = {2018},
}

%caption dataset
@article{DBLP:journals1,
  author       = {Ting{-}Yao Hsu and
                  C. Lee Giles and
                  Ting{-}Hao Kenneth Huang},
  title        = {SciCap: Generating Captions for Scientific Figures},
  journal      = {CoRR},
  volume       = {abs/2110.11624},
  year         = {2021},
  url          = {https://arxiv.org/abs/2110.11624},
  eprinttype    = {arXiv},
  eprint       = {2110.11624},
  timestamp    = {Thu, 28 Oct 2021 15:25:31 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2110-11624.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


%caption dataset
@article{DBLP:journals2,
  author       = {Bryan Wang and
                  Gang Li and
                  Xin Zhou and
                  Zhourong Chen and
                  Tovi Grossman and
                  Yang Li},
  title        = {Screen2Words: Automatic Mobile {UI} Summarization with Multimodal
                  Learning},
  journal      = {CoRR},
  volume       = {abs/2108.03353},
  year         = {2021},
  url          = {https://arxiv.org/abs/2108.03353},
  eprinttype    = {arXiv},
  eprint       = {2108.03353},
  timestamp    = {Tue, 23 Jul 2024 17:41:51 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2108-03353.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

% vqa dataset
@InProceedings{balanced_binary_vqa,
author = {Peng Zhang and Yash Goyal and Douglas Summers{-}Stay and Dhruv Batra and Devi Parikh},
title = {{Y}in and {Y}ang: Balancing and Answering Binary Visual Questions},
booktitle = {Conference on Computer Vision and Pattern Recognition (CVPR)},
year = {2016},
}

% roco dataset
@techreport{pelka2018roco,
  title = {Radiology Objects in Context (ROCO): A Multimodal Image Dataset},
  author = {Obioma Pelka and Sven Koitka and Johannes R\"uckert and Felix Nensa and Christoph M. Friedrich},
  institution = {University of Applied Sciences and Arts Dortmund, TU Dortmund University, University of Duisburg-Essen},
  year = {2018},
  url = {https://labels.tue-image.nl/wp-content/uploads/2018/09/AM-04.pdf},
  note = {Accessed: 2024-11-02}
}

% medpix dataset
@misc{siragusa2024medpix20comprehensivemultimodal,
      title={MedPix 2.0: A Comprehensive Multimodal Biomedical Dataset for Advanced AI Applications}, 
      author={Irene Siragusa and Salvatore Contino and Massimo La Ciura and Rosario Alicata and Roberto Pirrone},
      year={2024},
      eprint={2407.02994},
      archivePrefix={arXiv},
      primaryClass={cs.DB},
      url={https://arxiv.org/abs/2407.02994}, 
}

% scaling law
@misc{kaplan2020scalinglawsneurallanguage,
      title={Scaling Laws for Neural Language Models}, 
      author={Jared Kaplan and Sam McCandlish and Tom Henighan and Tom B. Brown and Benjamin Chess and Rewon Child and Scott Gray and Alec Radford and Jeffrey Wu and Dario Amodei},
      year={2020},
      eprint={2001.08361},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2001.08361}, 
}

% 4m
@inproceedings{4m,
    title={{4M}: Massively Multimodal Masked Modeling},
    author={David Mizrahi and Roman Bachmann and O{\u{g}}uzhan Fatih Kar and Teresa Yeo and Mingfei Gao and Afshin Dehghan and Amir Zamir},
    booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
    year={2023},
}

@article{4m21,
    title={{4M-21}: An Any-to-Any Vision Model for Tens of Tasks and Modalities},
    author={Roman Bachmann and O{\u{g}}uzhan Fatih Kar and David Mizrahi and Ali Garjani and Mingfei Gao and David Griffiths and Jiaming Hu and Afshin Dehghan and Amir Zamir},
    journal={arXiv 2024},
    year={2024},
}

% another example of attention based saliency mapping for peumonia classification
@article{wollek2024attention,
  title = {Attention-based Saliency Maps Improve Interpretability of Pneumothorax Classification},
  author = {Wollek, Alessandro and Graf, Robert and ƒåeƒçatka, Sa≈°a and Fink, Nicola and Willem, Theresa and Sabel, Bastian O. and Lasser, Tobias},
  note = {A.W. and R.G. contributed equally to this work},
  affiliation = {
    Munich Institute of Biomedical Engineering and Department of Informatics, Technical University of Munich, Boltzmannstr 11, Garching b., Munich 85748, Germany (A.W., R.G., T.L.);
    Department of Radiology, University Hospital LMU, Munich, Germany (S.ƒå., N.F., B.O.S.);
    Munich School of Technology in Society, Technical University of Munich, Munich, Germany (T.W.)
  },
  year = {2024},
  address = {Address correspondence to A.W. (email: alessandro.wollek@tum.de)}
}

% medpalm
@article{Singhal2023,
  author    = {Karan Singhal and Shekoofeh Azizi and Tomas Tu and others},
  title     = {Large language models encode clinical knowledge},
  journal   = {Nature},
  volume    = {620},
  pages     = {172--180},
  year      = {2023},
  doi       = {10.1038/s41586-023-06291-2},
  url       = {https://doi.org/10.1038/s41586-023-06291-2},
  received  = {25 January 2023},
  accepted  = {05 June 2023},
  published = {12 July 2023},
  issue     = {03 August 2023}
}

% implementations of grad cam with vit
@misc{jacobgilpytorchcam,
  title={PyTorch library for CAM methods},
  author={Jacob Gildenblat and contributors},
  year={2021},
  publisher={GitHub},
  howpublished={\url{https://github.com/jacobgil/pytorch-grad-cam}},
}

% llava med
@misc{li2023llavamedtraininglargelanguageandvision,
      title={LLaVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day}, 
      author={Chunyuan Li and Cliff Wong and Sheng Zhang and Naoto Usuyama and Haotian Liu and Jianwei Yang and Tristan Naumann and Hoifung Poon and Jianfeng Gao},
      year={2023},
      eprint={2306.00890},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2306.00890}, 
}

% said nice things about multimodality
@article{ryai_2019180031,
  author       = {Prevedello, Luciano M. and Halabi, Safwan S. and Shih, George and Wu, Carol C. and Kohli, Marc D. and Chokshi, Falgun H. and Erickson, Bradley J. and Kalpathy-Cramer, Jayashree and Andriole, Katherine P. and Flanders, Adam E.},
  title        = {Challenges Related to Artificial Intelligence Research in Medical Imaging and the Importance of Image Analysis Competitions},
  journal      = {Radiology: Artificial Intelligence},
  volume       = {1},
  number       = {1},
  pages        = {e180031},
  year         = {2019},
  doi          = {10.1148/ryai.2019180031},
  note         = {PMID: 33937783},
  url          = {https://doi.org/10.1148/ryai.2019180031},
  eprint       = {https://doi.org/10.1148/ryai.2019180031},
  abstract     = {In recent years, there has been enormous interest in applying artificial intelligence (AI) to radiology. Although some of this interest may have been driven by exaggerated expectations that the technology can outperform radiologists in some tasks, there is a growing body of evidence that illustrates its limitations in medical imaging. The true potential of the technique probably lies somewhere in the middle, and AI will ultimately play a key role in medical imaging in the future. The limitless power of computers makes AI an ideal candidate to provide the standardization, consistency, and dependability needed to support radiologists in their mission to provide excellent patient care. However, important roadblocks currently limit the expansion of this field in medical imaging. This article reviews some of the challenges and potential solutions to advance the field forward, with focus on the experience gained by hosting image-based competitions. Keywords: Convolutional Neural Network (CNN), Diagnosis, Supervised learning.}
}

% gradcam
@article{Selvaraju_2019,
   title={Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization},
   volume={128},
   ISSN={1573-1405},
   url={http://dx.doi.org/10.1007/s11263-019-01228-7},
   DOI={10.1007/s11263-019-01228-7},
   number={2},
   journal={International Journal of Computer Vision},
   publisher={Springer Science and Business Media LLC},
   author={Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
   year={2019},
   month=oct, pages={336‚Äì359} }

% quantifying attention flow , rollout 
@misc{abnar2020quantifyingattentionflowtransformers,
      title={Quantifying Attention Flow in Transformers}, 
      author={Samira Abnar and Willem Zuidema},
      year={2020},
      eprint={2005.00928},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2005.00928}, 
}

% benchmarking saliency
@article{Saporta2022,
  author       = {Saporta, Adriel and Gui, Xiaotong and Agrawal, Ashwin and Pareek, Anuj and Truong, Steven Q. H. and Nguyen, Chanh D. T. and Ngo, Van-Doan and Seekins, Jayne and Blankenberg, Francis G. and Ng, Andrew Y. and Lungren, Matthew P. and Rajpurkar, Pranav},
  title        = {Benchmarking saliency methods for chest X-ray interpretation},
  journal      = {Nature Machine Intelligence},
  volume       = {4},
  number       = {10},
  pages        = {867--878},
  year         = {2022},
  month        = {October},
  doi          = {10.1038/s42256-022-00536-x},
  url          = {https://doi.org/10.1038/s42256-022-00536-x},
  issn         = {2522-5839},
  abstract     = {Saliency methods, which produce heat maps that highlight the areas of the medical image that influence model prediction, are often presented to clinicians as an aid in diagnostic decision-making. However, rigorous investigation of the accuracy and reliability of these strategies is necessary before they are integrated into the clinical setting. In this work, we quantitatively evaluate seven saliency methods, including Grad-CAM, across multiple neural network architectures using two evaluation metrics. We establish the first human benchmark for chest X-ray segmentation in a multilabel classification set-up, and examine under what clinical conditions saliency maps might be more prone to failure in localizing important pathologies compared with a human expert benchmark. We find that (1) while Grad-CAM generally localized pathologies better than the other evaluated saliency methods, all seven performed significantly worse compared with the human benchmark, (2) the gap in localization performance between Grad-CAM and the human benchmark was largest for pathologies that were smaller in size and had shapes that were more complex, and (3) model confidence was positively correlated with Grad-CAM localization performance. Our work demonstrates that several important limitations of saliency methods must be addressed before we can rely on them for deep learning explainability in medical imaging.}
}



% saliency aware modelling
@INPROCEEDINGS{aware_saliency,
  author={Li, Hui and Wang, Luxi and Li, Yingming},
  booktitle={2023 IEEE International Conference on Visual Communications and Image Processing (VCIP)}, 
  title={Efficient Context and Saliency Aware Transformer Network for No-Reference Image Quality Assessment}, 
  year={2023},
  volume={},
  number={},
  pages={1-5},
  abstract={No-Reference Image Quality Assessment (NR-IQA) aims to estimate the perceptual image quality without access to reference images. To deal with it effectively and efficiently, in this work we propose a Context and Saliency aware Transformer Network (CSTNet), which is built based on a lightweight pyramid Vision Transformer (ViT). Specifically, a Multi-scale Context Aware Refinement (MCAR) block is devised to fully leverage hierarchical context features extracted by the ViT backbone. Further, saliency map prediction is incorporated as a sub-task to simulate the human attention on salient regions when perceiving images. Extensive experiments on public image quality datasets demonstrate its efficiency and superiority compared to the state-of-the-art models.},
  keywords={Image quality;Context-aware services;Visual communication;Transformers;Feature extraction;image quality assessment;transformer;saliency},
  doi={10.1109/VCIP59821.2023.10402637},
  ISSN={2642-9357},
  month={Dec},}


% pmc15m
@misc{zhang2024biomedclipmultimodalbiomedicalfoundation,
      title={BiomedCLIP: a multimodal biomedical foundation model pretrained from fifteen million scientific image-text pairs}, 
      author={Sheng Zhang and Yanbo Xu and Naoto Usuyama and Hanwen Xu and Jaspreet Bagga and Robert Tinn and Sam Preston and Rajesh Rao and Mu Wei and Naveen Valluri and Cliff Wong and Andrea Tupini and Yu Wang and Matt Mazzola and Swadheen Shukla and Lars Liden and Jianfeng Gao and Matthew P. Lungren and Tristan Naumann and Sheng Wang and Hoifung Poon},
      year={2024},
      eprint={2303.00915},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2303.00915}, 
}

% pmc vqa for curating datasets
@misc{zhang2024pmcvqavisualinstructiontuning,
      title={PMC-VQA: Visual Instruction Tuning for Medical Visual Question Answering}, 
      author={Xiaoman Zhang and Chaoyi Wu and Ziheng Zhao and Weixiong Lin and Ya Zhang and Yanfeng Wang and Weidi Xie},
      year={2024},
      eprint={2305.10415},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2305.10415}, 
}

%mimic cxr
@misc{johnson2019mimiccxrjpglargepubliclyavailable,
      title={MIMIC-CXR-JPG, a large publicly available database of labeled chest radiographs}, 
      author={Alistair E. W. Johnson and Tom J. Pollard and Nathaniel R. Greenbaum and Matthew P. Lungren and Chih-ying Deng and Yifan Peng and Zhiyong Lu and Roger G. Mark and Seth J. Berkowitz and Steven Horng},
      year={2019},
      eprint={1901.07042},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1901.07042}, 
}

%pubmedclip [they had the idea of gradcam]
@misc{eslami2021doesclipbenefitvisual,
      title={Does CLIP Benefit Visual Question Answering in the Medical Domain as Much as it Does in the General Domain?}, 
      author={Sedigheh Eslami and Gerard de Melo and Christoph Meinel},
      year={2021},
      eprint={2112.13906},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2112.13906}, 
}

%% reference to calculate saliency map
% https://github.com/jeonsworld/ViT-pytorch/blob/main/visualize_attention_map.ipynb

% treemaps as a good vis
@article{article_treemap,
author = {Long, Lim and Hui, Lim and Fook, Gim and Zainon, Wan Mohd Nazmee},
year = {2017},
month = {01},
pages = {108-115},
title = {A Study on the Effectiveness of Tree-Maps as Tree Visualization Techniques},
volume = {124},
journal = {Procedia Computer Science},
doi = {10.1016/j.procs.2017.12.136}
}